<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Model Review Notes</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/journal.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 61px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 66px;
  margin-top: -66px;
}

.section h2 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h3 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h4 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h5 {
  padding-top: 66px;
  margin-top: -66px;
}
.section h6 {
  padding-top: 66px;
  margin-top: -66px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Lending Club Model</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Analysis
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Planning</li>
    <li>
      <a href="ResearchQuestions.html">Research Questions</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Prior Research Review</li>
    <li>
      <a href="LiteratureReview.html">Literature</a>
    </li>
    <li>
      <a href="ModelReviewNotes.html">Models</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Exploratory Analysis</li>
    <li>
      <a href="LoanAmount.html">Loan Amount</a>
    </li>
    <li>
      <a href="LoanGrades.html">Loan Grades</a>
    </li>
    <li>
      <a href="DebtToIncome.html">Debt-To-Income</a>
    </li>
    <li>
      <a href="Income.html">Income</a>
    </li>
    <li>
      <a href="LoanPurpose.html">Loan Purpose</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Modeling</li>
    <li>
      <a href="Modeling.html">Model</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Related Packages
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="http://ryankuhn.net/LendingClub">LendingClub</a>
    </li>
    <li>
      <a href="http://ryankuhn.net/LendingClubData">LendingClubData</a>
    </li>
  </ul>
</li>
<li>
  <a href="http://ryankuhn.net">ryankuhn.net</a>
</li>
<li>
  <a href="https://github.com/kuhnrl30/LendingClubModel">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Model Review Notes</h1>

</div>


<blockquote>
<p>NOTE: this file is being used to capture my notes while reading through the model writeups. I’ll turn this into a cohesive summary once my analysis is complete.</p>
</blockquote>
<div id="purpose" class="section level1">
<h1>Purpose</h1>
<p>To compile a list of analyses already performed on the open source data and to learn the interesting relationships discovered in those analyses.</p>
<div id="lending-club---predicting-loan-outcomes" class="section level2">
<h2>Lending club - predicting loan outcomes</h2>
<p><span class="citation">ORourke (2016)</span></p>
<p>Data Period: 2007-2011</p>
<p><strong>Author Notes</strong>:</p>
<ul>
<li><a href="https://www.linkedin.com/in/ted-orourke/">LinkedIn profile</a> shows NO background in p2p lending or credit markets.<br />
</li>
<li>Education at the University of Virginia not yet completed.</li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li>Divided characteristics into two categories: those about the loan and about the borrower. Did not formally define the groups.</li>
<li>Filtered out late loans leaving just fully paid, defaulted, and charged off. Defaulted and charge-off loans combined into a single group.<br />
</li>
<li>Filtered characteristics by reading the descriptions in the data dictionary. Criteria seems to be intuition and the level of documentation. There was no analysis for correlation with outcome to support decisions.</li>
<li>Chargeoffs focused in higher interest rates, fully paid more evenly distributed<br />
</li>
<li>distribution of loans by grade<br />
</li>
<li>36 vs 60 month loan chargeoff rates<br />
</li>
<li>Current.Account.Ratio number of open credit lines divided by total number f accounts</li>
</ul>
<p><strong>Modeling:</strong></p>
<ul>
<li>Decision tree- The AUC from this model was 0.68 but only 60 charge-off loans were correctly identified. After resampling to increase the proportion of charge-offs, the confusion matrix increased to identifying 700 of the 1100 charge-offs.</li>
<li>Logistic Regression- AUC: 0.697. Confusion matrix ID’d 730 of the 1100 charge-offs.</li>
<li>Ensembled- Using bagging</li>
</ul>
<p><strong>Analysis review:</strong></p>
<ul>
<li>The exploratory analysis was not particularly insightful. The author did not document the basis for several key decisions such as which variables to include/exclude or why he chose the specific algorithms. The charts were not styled beyond the ggplot defaults.</li>
</ul>
<hr>
</div>
<div id="changes-in-lending-club-underwriting" class="section level2">
<h2>Changes in lending club underwriting</h2>
<p><span class="citation">Wu (2015)</span></p>
<p>Data: June 2015 - May 2016</p>
<p><strong>Notes</strong>:</p>
<ul>
<li>Response to LC’s quarterly earnings release, where LC announced changes in underwriting</li>
<li>Used commentary from LC’s 10-Q.</li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li>Scatter plot of average DTI for D grade loans by issue date. The plot attempts to show that less high DTI loans have been issued since LC changed its underwriting in late April 2016. The article hypothesizes that the loans are removed from the direct pay program. Direct pay requires the loan proceeds be paid directly to the borrower’s existing debts.</li>
<li>Scatter plot of number of inquiries by issue date for F and G grade loans. The plot shows that in the past month there have been less loans with a high number of previous inquiries.</li>
</ul>
<hr>
</div>
<div id="analysis-of-lending-clubs-data" class="section level2">
<h2>Analysis of Lending Club’s data</h2>
<p><span class="citation">Darre (2015)</span></p>
<p>Data: through June 2015.</p>
<p><strong>Author Notes</strong>:</p>
<ul>
<li>Strong educational background for data science: MSc in Statistics from Stanford University and MSc in Applied Math</li>
<li><a href="https://www.linkedin.com/in/jftdarre/">LinkedIn profile</a> shows work experience as a quant for UBS in structuring CDOs.</li>
</ul>
<p><strong>Data cleaning:</strong></p>
<ul>
<li>Removed loans where the :
<ul>
<li>policy code is not 1 meaning they are new products not yet publicly available.</li>
<li>high FICO score was 0 meaning the data was not valid.</li>
<li>Revolving credit utilization data was missing</li>
<li>Low FICO score was less than 600. Per the new underwriting policy, borrowers with a score less than 600 aren’t allowed to receive a loan.</li>
</ul></li>
<li>New features engineered:
<ul>
<li>Year from the issue date</li>
<li>Buckets for the:
<ul>
<li>FICO scores</li>
<li>years, quarters, and months of issue dates</li>
<li>number of inquires in the past 6 months</li>
<li>DTI</li>
<li>Revolving balance</li>
</ul></li>
</ul></li>
<li>Filtered to remove loans that have not matured.</li>
<li>Grouped statuses into 2 categories: debt and purchases. Credit card and debt consolidation were grouped into the debt category and all others to purchases.</li>
<li>Simplified the number of delinquencies to binary: 2+ delinquencies in 2 years.</li>
<li>Simplified the number of public records to binary: threshold +1.</li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li>FICO score by loan grade showing that most subgrades have average FICO scores within a 10 point range. The initial conclusion is that FICO score doesn’t have a linear relationship with the loan grade. Expanded to show density around the average score by year which shows the densities started with a tight distribution around the mean and then widened with time. The ending conclusion is that LC initially relied heavily on the credit score in the loan grading model and then loosened their reliance on it.</li>
<li>Look at several variables for a relationship with defaults, loan grades, and fico scores:</li>
</ul>
<table>
<colgroup>
<col width="25%" />
<col width="75%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Variable Name</th>
<th>Conclusion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Home ownership</td>
<td>‘mortgage’ defaulted at 11.5% and 2% less than other values. No analysis by loan grades</td>
</tr>
<tr class="even">
<td align="left">Purpose</td>
<td>‘education’ and ‘small business’ have higher charge off rates but education is a very small population. At the total population level, the FICO score does not reflect the higher risk</td>
</tr>
<tr class="odd">
<td align="left">Revolving balance &amp; employment length</td>
<td>no or minimal link to default. A bump in the charge-off ratio when the employment length is NA which is in contrast to the higher average credit score.</td>
</tr>
<tr class="even">
<td align="left">Number of delinquencies</td>
<td>Majority of borrowers do not have delinquencies leaving a small population which did. Delinquencies did have a lower average credit grade.</td>
</tr>
<tr class="odd">
<td align="left">Number of open accounts</td>
<td>No clear trend</td>
</tr>
<tr class="even">
<td align="left">Debt to Income</td>
<td>Author notes a relationship with dti and higher credit score but that does not hold true for scores below 700.</td>
</tr>
<tr class="odd">
<td align="left">Public Records</td>
<td>Small population but a 2% increase in charge-offs if already had a public record.</td>
</tr>
<tr class="even">
<td align="left">Age of credit history</td>
<td>Default rates decrease with credit history age. There was a 5% drop in defaults from borrowers with &lt;6 years to &gt;22 years. A secondary analysis could be performed to show the impact of borrower age on credit history age.</td>
</tr>
<tr class="odd">
<td align="left">Revolving utilization</td>
<td>Higher usage leads to higher default rates</td>
</tr>
<tr class="even">
<td align="left">Annual income</td>
<td>High income defaults less, but does not eliminate the risk. Likely a significant variable.</td>
</tr>
<tr class="odd">
<td align="left">Inquiries in 6 months</td>
<td>An indication of desperation to get credit. 7+ inquires are nearly 4x more likely to default than 0 inquiries.</td>
</tr>
<tr class="even">
<td align="left">Geography</td>
<td>Nevada and Florida are top 2 states for defaults.</td>
</tr>
</tbody>
</table>
<p>Model Review:</p>
<ul>
<li>A very comprehensive analysis with several approaches that can be updated and reused. Use of violin charts to show the distribution of credit scores was very effective and can be used for dti, income, and credit score analysis. Plotly pie charts were less effective in transferring information because the default rates by category were embedded in the pop-ups instead of being directly visualized. Having just read Tufte’s work, I think the bubble charts were too large to convey so little information and may have been better suited in a table. The analysis was a little disjointed the way he moved between the complete dataset and the subset of matured loans. It may have been useful to do them sequentially or enable a left-right comparison.</li>
</ul>
<hr>
</div>
<div id="lending-club-loan-analysis-making-money-with-logistic-regression" class="section level2">
<h2>Lending Club Loan Analysis: Making Money with Logistic Regression</h2>
<p><span class="citation">Davis (2012)</span></p>
<p>Data: through 2011</p>
<p><strong>Author Notes</strong>:</p>
<ul>
<li><a href="https://www.linkedin.com/in/drjasondavis/?locale=en_US">LinkedIn</a> shows at the time he was the director of Search and Data at Etsy</li>
<li>PhD in machine learning U of TX- Austin</li>
</ul>
<p><strong>Notes</strong>:</p>
<ul>
<li>Interesting discussion in the comments</li>
</ul>
<p><strong>Analysis:</strong></p>
<ul>
<li>Default rate by description character length. He did not define how he measured defaults i.e. did he remove loans that haven’t matured yet or were defaults included with the charge-offs.</li>
<li>Logistic regression using first 50% of loans and evaluated on 2nd 50%. The model output is the probability of default which is then used to weight the interest rate. 12 variables were used in the model with no commentary discussing how they were chosen.</li>
<li>Expanded to determine sensitivity of each of the variables by re running the model after holding out one of the variables. Noted that the amount requested had the biggest impact on investor return.</li>
</ul>
<p><strong>Review:</strong> - An interesting take on the model by flowing it through to a pricing model. So far in my review the authors have stopped at a classifier model. This is an important shift towards application.</p>
<hr>
</div>
<div id="analyzing-historical-default-rates-of-lending-club-notes" class="section level2">
<h2>Analyzing Historical Default Rates of Lending Club Notes</h2>
<p><span class="citation">Toth (2015)</span></p>
<p>Data: 2012-2013</p>
<p>Author Notes:</p>
<ul>
<li>Data scientist at Orchard, a ‘provider of data, tech and software to the online lending industry’.</li>
<li>Undergrad at Wharton in stats/finance and a minor in math</li>
</ul>
<p>Data Cleaning: - Classified statuses into performing and non-performing. Current and fully paid went into the performing group and all others to non-performing. - Removed variables that would not have been known at the time of issuance. -</p>
<p>Analysis:</p>
<ul>
<li>Share of loans performing/non-performing by grade and a further break down by subgrade</li>
<li>Variable analysis:</li>
</ul>
<table>
<colgroup>
<col width="25%" />
<col width="75%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Variable Name</th>
<th>Conclusion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Home ownership</td>
<td>prop.test for significance of mortgage vs owners and another for owners vs renters. Both tests were significant.</td>
</tr>
<tr class="even">
<td align="left">Debt to Income</td>
<td>Bucketed by 10% increments and intuitively states defaults increase as dti increased.</td>
</tr>
<tr class="odd">
<td align="left">Revolving Utilization</td>
<td>Bucketed by 10% increments and intuitively states defaults increase as utilization increased.</td>
</tr>
<tr class="even">
<td align="left">Purpose</td>
<td>Calculated stats for each loan but summarized as 3 categories: 1) debt, 2) major purchase such as home improvement and cars, 3) luxury such as vacations and weddings.</td>
</tr>
<tr class="odd">
<td align="left">Inquiries</td>
<td>Curve peaking at 3 inq and decreasing again with 4+ inquiries.</td>
</tr>
<tr class="even">
<td align="left">Total accounts</td>
<td>He cites a curve peaking at 20 accounts and diminishing effects above that. There’s no analysis documenting the the bin size choice.</td>
</tr>
<tr class="odd">
<td align="left">Annual Income</td>
<td>Bin by quintiles- top 20% of annual income are 6% more likely to be performing loans. The top cutoff for the top and bottom quintiles was 42K and 95K.</td>
</tr>
<tr class="even">
<td align="left">Loan Amount</td>
<td>Binned at 0-15,15-30, and 30-35K. Noting a decrease in performance for 30-35K loans. <em>This contrasts with numbers showing in my Loan Amount exploratory analysis?</em></td>
</tr>
<tr class="odd">
<td align="left">Employment Length</td>
<td>Binned as NA, &lt;10, &gt;10 years. No commentary but data suggests 1.25% improvement for &gt;10 years over &lt;10 years and another 1.25% over NAs.</td>
</tr>
<tr class="even">
<td align="left">Delinquencies past 2 years</td>
<td>Binned to 0, 1, 2, 3+ based on tail of distribution. No difference in 0-2 delinquencies but a fall-off of loan performance at 3+.</td>
</tr>
<tr class="odd">
<td align="left">Number Open Accounts</td>
<td>No strong indication</td>
</tr>
<tr class="even">
<td align="left">Verifed Income Statements</td>
<td>Unexpected worse performance as verification status increased. Author suggests confounding variable as verification increases as credit score degraded.</td>
</tr>
<tr class="odd">
<td align="left">Public Records</td>
<td>0, 1, 2+. Better performance at 1+ than 0</td>
</tr>
<tr class="even">
<td align="left">Non-Significant Variables</td>
<td>Months since last delinquencies, months since last major derogatory note, collections in past 12 months</td>
</tr>
</tbody>
</table>
<p><strong>Review</strong>: Extensive analysis of many variables. This variable-by-variable format has been used in other blogs but can be very boring to read without a central theme. The variables don’t build on each other or follow a line of reasoning. Is there a better way? Also, there is no analysis of interaction between variables.</p>
<p>Author proposes a hypotheses for differences in results by variables but there’s no testing for confirmation. (and how would you confirm?) Also, minimal documentation around decisions like bin cutoffs.</p>
<hr>
</div>
<div id="gradient-boosting-analysis-of-lendingclubs-data" class="section level2">
<h2>Gradient Boosting: Analysis of LendingClub’s Data</h2>
<p><span class="citation">Davenport (2013)</span></p>
<p>Data: Not stated, but likely through Q1 2013.</p>
<p>Author Notes:</p>
<ul>
<li>Data scientist at Amazon<br />
</li>
<li>MS Computer Science from Georgia Tech</li>
</ul>
<p>Data Cleaning:</p>
<ul>
<li>Replace missing values with NA</li>
<li>Anova analysis listed Amount Requested, Debt/Income Ratio, Rent or Own Home, Inquiries in Last 6 Months, Length of Loan, and Purpose of Loan as being significantly correlated with MeanFICO and Interest Rate.</li>
<li>Looked to the credit score info to understand link to other variables</li>
</ul>
<p>Modeling:</p>
<ul>
<li>Objective: predict interest rate using RMSE as accuracy measure<br />
</li>
<li>Straight to gbm: used to prevent overfitting</li>
</ul>
<p>Analysis:</p>
<p>The author presents a fairly simple analysis which has more to do with interpreting the results of the gbm function than actually predicting the interest rates. This model has minimal utility in predicting default probability or expected cash flow from a loan. There are two possible insights from the blog post:</p>
<ol style="list-style-type: decimal">
<li>Loans have already been priced by the time they are on the market but it may be useful to understanding how loans were priced. You could then make a local optimization argument that there are arbitration gains where the interest rate doesn’t reflect risk. The author did not attempt to explore, much less prove, that line of thinking.</li>
<li>Composition of the FICO score is a combination of amounts owed, payment history, credit mix, length of credit history, and new credit. This is a good starting point to identify confounding or high correlation variables.</li>
</ol>
<hr>
</div>
<div id="lending-club-data-analysis-revisited-with-python" class="section level2">
<h2>Lending Club Data Analysis Revisited with Python</h2>
<p><span class="citation">Davenport (2015)</span></p>
<p>Data:</p>
<p>Author:</p>
<ul>
<li>see above</li>
</ul>
<p>Cleaning:</p>
<ul>
<li>Question relationships to identify risks, but no proposed solutions or conclusions without confirmation or analysis.</li>
<li>Exploratory analysis was generally just distributions<br />
</li>
<li>ID’d these risks:</li>
</ul>
<table>
<colgroup>
<col width="25%" />
<col width="75%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Variable Name</th>
<th>Risk</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Employment Title</td>
<td>Too many unique values to include as variable. Need to group somehow.</td>
</tr>
<tr class="even">
<td align="left">Employment Length</td>
<td>May confuse buying power, Job tenure doesn’t indicate purchasing power.</td>
</tr>
<tr class="odd">
<td align="left">Earliest credit line</td>
<td>Longer period could indicate age and earning potential.</td>
</tr>
</tbody>
</table>
<p>Modeling:</p>
<ul>
<li>Objective was to explain the differences in interest rate</li>
<li>Removed correlated variables &gt;0.55</li>
<li>Used gradient boosting regression, tuned with grid search</li>
<li>Accuracy gets better with more iterations. Is there an over fitting concern?</li>
</ul>
<p>Analysis: This is the first model that mentioned the risks of being wrong instead of being assertive towards assumed explanations. Most other models had been cavalier towards their hypotheses around deviances in the variables.</p>
<p>Putting the Python code in the analysis was a bit distracting. While it’s good for reproducibility, it took away from the goal of understanding the author’s message and what’s happening with the loans. I think its preferable to include a link to Github or hold everything to the end.</p>
<hr>
</div>
<div id="lending-club-deep-learning" class="section level2">
<h2>Lending Club Deep Learning</h2>
<p><span class="citation">Summers (2016)</span></p>
<p>Data: 2015 &amp; 1H 2016</p>
<p>Modeling:</p>
<ul>
<li>Defined objective as “Use machine learning model to create a portfolio of 100 loans with a higher return than a Lending Club baseline”. Note this is stated as measuring against profitability- shouldn’t be just a default score. Used an average 10% return for the total population- no hold out.</li>
<li>Links to external analyses- less duplicated work.</li>
<li>Three strategies:</li>
</ul>
<table>
<colgroup>
<col width="11%" />
<col width="88%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Strategy</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td>Model assigns the probability of default. Scans through the range of probabilities calculating the average return, interest rate, and default rate for all loans above the cutoff. Find point of maximum return. Risk rate is the actual default rate. Model reduces risk of default but doesn’t compensate for lower interest rate.</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td>Same model as above but applied to subsets of loans by grade. The model shows minimal improvement over a random sample from A &amp; B grades but the results improve as the grades increase.</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td>Change optimization from focusing on default rates to total return. Weight the payback probability by the interest rate. Minimal discussion, but the return percentage is higher than approach #1.</td>
</tr>
</tbody>
</table>
<ul>
<li>The margin over average LC returns was highest in approach 3 but 2 applied to higher interest rate loans would be the best overall return. Approach number 3 maximizes the risk adjusted return.<br />
</li>
<li>Used 10 fold cross validation</li>
</ul>
<p>Analysis: Approach was written to maximize profit but the objective is written as a machine learning task. He starts as a demo type post but then does a better job of framing the writeup like a business driven model.</p>
<p>Author lets the code do much of the talking which makes interpreting the model difficult since my Python is not as strong. Recommend using a more verbose commentary to let non-Python readers understand what is happening in the code.</p>
<hr>
</div>
<div id="mining-lending-clubs-goldmine-of-loan-data-part-i-of-ii-visualizations-by-state" class="section level2">
<h2>Mining Lending Club’s Goldmine of Loan Data Part I of II – Visualizations by State</h2>
<p><span class="citation">Cashorali (2011)</span></p>
<p>Data: Before Q3 2011</p>
<p>Author:</p>
<ul>
<li>BS in computer science and biology from Northeastern<br />
</li>
<li>Worked in various analytics &amp; data science roles starting in 2012 (1 year after the post was written)</li>
</ul>
<p>Cleaning:</p>
<ul>
<li>Visualize the number of loans by state- No loans issued in North Dakota</li>
<li>Animation of interest rates by year, state. Showed that rates were heterogeneous in 2007 than in 2011.</li>
<li>Looked at fully paid vs late/defaulted loans by state. Florida had 40% default rate. TX, PA and NJ had 20% performing loan rate.</li>
</ul>
<p>Analysis: Fairly superficial analysis but the first, so far, to consider geography. Interesting to note that FL and CA had high default rates but TX, PA, and NJ were the lower rates.</p>
<hr>
</div>
<div id="investing-at-lending-club-with-watson-analytics" class="section level2">
<h2>Investing at Lending Club with Watson Analytics</h2>
<p><span class="citation">Polena (2016)</span><br />
Data: Jan 2009- Dec 2012</p>
<p>Author: - Was an analyst at IBM at the time of the post. - Master’s in quantitative economics - Master’s in Financial Markets and Banking</p>
<p>Cleaning: - Nothing described in the article</p>
<p>Modeling:</p>
<ul>
<li>Passed the data to Watson, asked questions of it.</li>
<li>Three insights:</li>
</ul>
<table style="width:99%;">
<colgroup>
<col width="11%" />
<col width="87%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">No</th>
<th>Insight</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td>Wyoming had the lowest default rate. The average rate by state was 14% but WY was only 9%.</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td>Lowest default rates by purpose are Weddings and Cars. Small business has the worst rates. The proposals that weddings and cars or better performing are at odds with other analyses reviewed.</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td>Employment length less than 5 years was more likely to repay than &gt;5 years. No code or visualizations to support the claim</td>
</tr>
</tbody>
</table>
<p>Analysis: Claims were not supported by code to reproduce the analysis or understand the exact assumptions and methods used. Claims were also not supported by a train of evidence and assumptions leading up the the final conclusion– the autor just cuts straight to the point. That may be useful as a demo of Watson’s capabilities but not from an analytic reference.</p>
<hr>
</div>
<div id="application-of-survival-analysis-to-p2p-lending-club-loans-data" class="section level2">
<h2>Application of survival analysis to P2P Lending Club loans data</h2>
<p><span class="citation">Mistry (2016)</span></p>
<p>Data: Not explicitly stated but assumed through Q2 2016</p>
<p>Author:<a href="https://www.linkedin.com/in/hitesh-mistry-1ba60121">LinkedIn</a> profile shows</p>
<ul>
<li>Clinical researcher and modeling</li>
<li>University of Manchester:
<ul>
<li>PhD in applied Math<br />
</li>
<li>Master’s in Applied Computing<br />
</li>
<li>Bachelor in Math</li>
</ul></li>
</ul>
<p>Cleaning:</p>
<ul>
<li>Do not have to exclude current loans if using survival analysis.</li>
<li>Set objective RR as the ratio of amount paid to amount lent. A value at the end of the loan greater than 1 is profitable and less than 1 is unprofitable.</li>
<li>Referred to yhat blog for useful covariates.</li>
<li>Looked for relationship of RR to the FICO score. Higher profitability score for FICO over 700.</li>
</ul>
<p>Modeling:</p>
<ul>
<li>Stratified by FICO</li>
<li>Used concordance rather than p-value to determine ranking of variables. Final model uses interest rate and term of the loan.</li>
<li>Simple survival analysis resulting in profit probability curves. Give the probability of achieving a range of profitability ratios.</li>
</ul>
<p>Analysis: Model gives a probability of reaching a given profitability ratio. This ratio isn’t the same as the expected Annualized return but could probably be reworked to calculate for as such. The post was written simple enough to be understandable without knowing the math behind the survival analysis- no proofs or equations. Contrarily, there was no code to supporting so it’s less reproducible. Seeing the shiny app as an output indicates it was done in R.</p>
<p>Concordance is a new concept and will need to be researched. Interesting to note that only interest rate and loan term were significant. These 2 seemed reasonable but I would have expected others to be significant.</p>
<hr>
</div>
<div id="survival-prediction-p2p-loan-profitability-competitions" class="section level2">
<h2>Survival prediction (P2P loan profitability) competitions</h2>
<p><span class="citation">Mistry (2017)</span></p>
<p>Suggested healthcare for survival analysis research. No model built in this post. Survival is used often to model disease states and how long a patient can live given a treatment variable. This is well developed science in healthcare and could be an opportunity to port over to finance specifically P2P lending.</p>
<hr>
</div>
<div id="using-genetic-algorithms-to-maximize-lending-club-performance" class="section level2">
<h2>Using genetic algorithms to maximize Lending Club performance</h2>
<p><span class="citation">Patierno (2011)</span></p>
<p>Author:</p>
<ul>
<li>Software engineer at Google</li>
<li>BS in Computer Science</li>
</ul>
<p>Data: Written in Feb 2011 and claiming to analyze 4 years of data. Assuming through Q4 2010.</p>
<p>Modeling:</p>
<ul>
<li>Used genetic algorithm to select attributes. No exploratory analysis. No test or hold out dataset used to validate and prevent overfitting.</li>
<li>Model source code on <a href="https://github.com/dmpatierno/LCBT">GitHub</a></li>
<li>Claimed to result in a 12.5% return</li>
</ul>
<p>Analysis: Author created a tool rather than a model. There are no conclusions for how to maximize returns, reduce defaults, or any other objective.</p>
<hr>
</div>
<div id="peer-lending-risk-predictor" class="section level2">
<h2>Peer Lending Risk Predictor</h2>
<p><span class="citation">Tsai, Ramiah, and Singh (2014)</span></p>
<p>Authors: Students at Stanford, advised by Andrew Ng</p>
<p>Data: 2007- 2013</p>
<p>Cleaning:</p>
<ul>
<li>Removed loans that were fully paid or defaulted. Similar to filtering for matured loans but would include early pay-offs and early defaults. (Does this change the result?)</li>
<li>Rebalanced the training set to be 50/50 defaulted vs paid loans. No discussion on how.
<ul>
<li>Feature selection using information gain in Weka and Matlab. Multiple methods consistently identified interest_rate, loan_amount, annual_income, and loan_purpose.</li>
</ul></li>
<li>Definition: Recall is number of loans paid and identified as paid</li>
<li>Definition: Precision is number of loans correctly identified as not performing</li>
<li>Used term precision to describe the amount of loans that were fully paid vs total loans in the category.</li>
</ul>
<p>Modeling:</p>
<ul>
<li>Linear regression- Penalty function added to logistic regression to avoid classifying defaulted as paid off. Preference for precision over recall or overall accuracy. Penalty factor increased precision from 88.9% to 95.9%.</li>
<li>SVM- Minimal discussion, probably truncated for space- 5 page limit.</li>
<li>Naive Bayes- No discussion</li>
<li>Random Forest: training accuracy was high but was over fitting. Used out-of-bag error to measure success and minimized error at 37.25%.</li>
<li>TF-IDF- Ranked words by TF-IDF for both paid and defaulted loans. Comparison for words with high rank in one but low in the other. <em>steady</em> and <em>God</em> ranked higher in loans that defaulted but <em>university</em> ranked higher in paid loans. Created binary variables for each an improved accuracy by 3%. [Are descriptions even required anymore?]</li>
<li>PCA for visualization. Plotting loans by 1st and 2nd components. Running a logistic regression to create a decision boundary. Eliminating defaulted loans also eliminates most good loans- hence high precision but low recall again.</li>
</ul>
<p>Conclusions:</p>
<ul>
<li>Acknowledged that models recommended only 0.6% of loans and wasn’t practical</li>
<li>Compare to LC: Used Modified LogReg to create portfolio with same risk profile as A1 subgrade but 2% higher return. Similarly compared model classifications to LC grades. Concludes LC is classifying high quality loans into lower credit grades/high interest. Opportunity knocks.</li>
<li>Understand model sensitivity to population skew, rebalance test set as needed. Generally, know your algorithms assumptions and weeknesses.</li>
<li>Achieved greatest success by using the penalty factor for incorrectly idenifying bad loans.</li>
</ul>
<p>Analysis: The first 3/4 of the paper goes through several models and measuring performance. Ng’s influence is apparent in the theoretical commentary and mathematical notation. The last page of the paper compares the best model against LC. Use their model to create similar risk tranches as LC, but with higher interest rates. This paper is the first I’ve read which quanifies the profitability impact of reducing defaulted loans. It is also the first to include a penalty function in the model- other papers have accepted models out of the box with no/minimal tuning.</p>
<p>I appreciated the idea of optimizing for precision instead of accuracy. Investment dollars are limited and at this point in my portfolio I am more likely to run out of funds before running out of investment opportunities. It is acceptable to significantly reduce the number of loans. I assume this is the case until a single investor is purchasing a material share of the market- say &gt; 10%.</p>
<p>This seems to be a recurring project in the course as there are papers for the <a href="http://cs229.stanford.edu/proj2011/JunjieLiang-PredictingBorrowersChanceOfDefaultingOnCreditLoans.pdf">2011</a>, <a href="http://cs229.stanford.edu/proj2014/Marie-Laure%20Charpignon,%20Enguerrand%20Horel,%20Flora%20Tixier,%20Prediction%20of%20consumer%20credit%20risk.pdf">2014</a>, <a href="http://cs229.stanford.edu/proj2015/199_report.pdf">2015</a> and <a href="http://cs229.stanford.edu/proj2016spr/report/039.pdf">2016</a> sessions.</p>
<hr>
</div>
<div id="lending-club-default-analysis" class="section level2">
<h2>Lending Club Default Analysis</h2>
<p><span class="citation">Moy (2015)</span></p>
<p>Author:</p>
<ul>
<li>Attended NYC Data Academy in 2015 at the time the post was written</li>
<li>MS in Operations Research from SUNY</li>
<li>Worked as Industrial Engineer prior to writing. No prior experience with p2p loans</li>
</ul>
<p>Data: 2007-2014</p>
<p>Cleaning:</p>
<ul>
<li>Removed loans not in current loan listing<br />
</li>
<li>New feature: character count for employee title</li>
<li>Removed ‘Current loans’, no discussion on how to handle late loans</li>
</ul>
<p>Modeling:</p>
<ul>
<li>Objective: predict default and reduce 22% loss on investments</li>
<li>Feature selection based on random forest, selecting variables above 20 ‘importance’. Importance was not defined.</li>
<li>Did not remove interest rate</li>
</ul>
<hr>
</div>
</div>
<div id="remaining-models" class="section level1">
<h1>Remaining Models</h1>
<p><a href="http://andirog.blogspot.com/2012/10/lending-club-loan-purpose-default-rate.html" class="uri">http://andirog.blogspot.com/2012/10/lending-club-loan-purpose-default-rate.html</a><br />
<a href="http://blog.yhat.com/posts/machine-learning-for-predicting-bad-loans.html" class="uri">http://blog.yhat.com/posts/machine-learning-for-predicting-bad-loans.html</a></p>
<ul>
<li><span class="citation">Gupta (2012)</span></li>
<li><span class="citation">Anon. (2015)</span><br />
</li>
<li><span class="citation">Anon.</span></li>
</ul>
</div>
<div id="trends-and-organization" class="section level1">
<h1>Trends and organization</h1>
<ul>
<li>3 tranches: ML demo, exploratory analysis, and practical application
<ul>
<li>ML demo: casual analysis and then quickly move to applying ML algorithm. Some feature engineering, but they don’t tend to be helpful variables. This category can be bifurcated into citizen data scientists doing demos of applied algos and academics using Greek letters to explain algos.</li>
<li>Exploratory: product of a course or bootcamp. Showing relationship of multiple variables. No resulting model or classifier.</li>
<li>Practical application: Blog posts from investment service firms. Show deep knowledge of the industry but are not a complete and comprehensive review of all variables. Tend to be a deep dive on a very narrow topic. Likely useful for understanding the nuance of the market but not for getting started.</li>
</ul></li>
<li>Approach for those with DS experience is to go variable by variable and look for the effect. Practitioners tend to focus o current events and the impact on the current credit scoring model.</li>
</ul>
<div id="major-take-aways" class="section level4">
<h4>Major take-aways:</h4>
<ul>
<li>Add a penalty function to adjust for loans falsely classified as performing. Optimize for precision</li>
<li>Model for profitability, not accuracy of the model</li>
<li>Reading a step-by-step analysis by variable is very boring but without the detail it’s hard to accept the conclusions. Should make sure to have a strong executive summary or abstract.</li>
<li>Follow intution- the expected variables are likely to have significance.<br />
</li>
<li>Interest is a measure of default risk and to reward the investor for taking that risk. Including int rate is going to be highly correlated with default experience so needs further thought. Looking for arbitrage from interest rate, not purely avoiding defaults.</li>
</ul>
</div>
</div>
<div id="references" class="section level1 unnumbered">
<h1>References</h1>
<div id="refs" class="references">
<div id="ref-socialLender">
<p>Anon., 2015, Lending club - predicting loan outcomes,. Blog <em>Peer Lending Server</em> (<a href="Analyzing Lending Club Payment History" class="uri">Analyzing Lending Club Payment History</a>).</p>
</div>
<div id="ref-mdlKaggle">
<p>Anon., Give me some credit,. Blog <em>Kaggle</em> (<a href="https://www.kaggle.com/c/GiveMeSomeCredit" class="uri">https://www.kaggle.com/c/GiveMeSomeCredit</a>).</p>
</div>
<div id="ref-mdlCashorali">
<p>Cashorali, Tanya, 2011, Mining lending club’s goldmine of loan data part i of ii visualizations by state,. Blog <em>R-bloggers</em> (<a href="https://www.r-bloggers.com/mining-lending-clubs-goldmine-of-loan-data-part-i-of-ii-visualizations-by-state/" class="uri">https://www.r-bloggers.com/mining-lending-clubs-goldmine-of-loan-data-part-i-of-ii-visualizations-by-state/</a>).</p>
</div>
<div id="ref-mdlDarre">
<p>Darre, Jean-Francois, 2015, Analysis of lending club’s data,. Blog <em>Data Science Central</em> (<a href="http://www.datasciencecentral.com/profiles/blogs/analysis-of-lending-club-s-data" class="uri">http://www.datasciencecentral.com/profiles/blogs/analysis-of-lending-club-s-data</a>).</p>
</div>
<div id="ref-mdlDavenport2">
<p>Davenport, Kevin, 2013, Gradient boosting: Analysis of lendingclub’s data,. Blog (<a href="http://kldavenport.com/gradient-boosting-analysis-of-lendingclubs-data//" class="uri">http://kldavenport.com/gradient-boosting-analysis-of-lendingclubs-data//</a>).</p>
</div>
<div id="ref-mdlDavenport">
<p>Davenport, Kevin, 2015, Lending club data analysis revisited with python,. Blog <em>Machine Learning and Statistics Blog</em> (<a href="http://kldavenport.com/lending-club-data-analysis-revisted-with-python/" class="uri">http://kldavenport.com/lending-club-data-analysis-revisted-with-python/</a>).</p>
</div>
<div id="ref-mdlDavis">
<p>Davis, Jason, 2012, Lending club loan analysis: Making money with logistic regression,. Blog <em>Data Startups</em> (<a href="http://drjasondavis.com/blog/2012/04/08/lending-club-loan-analysis-making-money-with-logistic-regression" class="uri">http://drjasondavis.com/blog/2012/04/08/lending-club-loan-analysis-making-money-with-logistic-regression</a>).</p>
</div>
<div id="ref-mdlGupta">
<p>Gupta, anil, 2012, Lending club loan purpose: Default rate and bad-loan experience index,. Blog <em>Random Thoughts</em> (<a href="http://andirog.blogspot.com/2012/10/lending-club-loan-purpose-default-rate.html" class="uri">http://andirog.blogspot.com/2012/10/lending-club-loan-purpose-default-rate.html</a>).</p>
</div>
<div id="ref-mdlMistry">
<p>Mistry, Hitesh, 2016, Application of survival analysis to p2p lending club loans data,. Blog <em>Systems Forecasting</em> (<a href="http://systemsforecasting.com/2016/07/application-of-survival-analysis-to-p2p-lending-club-loans-data/" class="uri">http://systemsforecasting.com/2016/07/application-of-survival-analysis-to-p2p-lending-club-loans-data/</a>).</p>
</div>
<div id="ref-mdlMistry2">
<p>Mistry, Hitesh, 2017, Survival prediction (p2p loan profitability) competitions,. Blog <em>Systems Forecasting</em> (http://systemsforecasting.com/2017/02/survival-prediction-p2p-loan-profitability-competitions/).</p>
</div>
<div id="ref-mdlMoy">
<p>Moy, Philip, 2015, Lending club default analysis, (<a href="http://www.creditreportservice.info/article/95227030/lending-club-default-analysis/" class="uri">http://www.creditreportservice.info/article/95227030/lending-club-default-analysis/</a>).</p>
</div>
<div id="ref-mdlOrourke">
<p>ORourke, Ted, 2016, Lending club - predicting loan outcomes,. Blog <em>Rpubs</em> (<a href="https://rpubs.com/torourke97/190551" class="uri">https://rpubs.com/torourke97/190551</a>).</p>
</div>
<div id="ref-mdlPatierno">
<p>Patierno, David, 2011, Using genetic algorithms to maximize lending club performance,. Blog (<a href="http://blog.dmpatierno.com/post/3161338411/lending-club-genetic-algorithm" class="uri">http://blog.dmpatierno.com/post/3161338411/lending-club-genetic-algorithm</a>).</p>
</div>
<div id="ref-mdlPolena">
<p>Polena, Michal, 2016, Investing at lending club with watson analytics,. Blog <em>IBM</em> (<a href="https://www.ibm.com/blogs/business-analytics/lending-club-and-watson-analytics-1/" class="uri">https://www.ibm.com/blogs/business-analytics/lending-club-and-watson-analytics-1/</a>).</p>
</div>
<div id="ref-mdlSummers">
<p>Summers, Cameron, 2016, Lending club deep learning, (<a href="https://scaubrey.github.io/" class="uri">https://scaubrey.github.io/</a>).</p>
</div>
<div id="ref-mdlToth">
<p>Toth, Michael, 2015, Analyzing historical default rates of lending club notes,. Blog <em>R-bloggers</em> (<a href="http://michaeltoth.me/analyzing-historical-default-rates-of-lending-club-notes.html" class="uri">http://michaeltoth.me/analyzing-historical-default-rates-of-lending-club-notes.html</a>).</p>
</div>
<div id="ref-mdlTsai">
<p>Tsai, Kevin, Sivagami Ramiah, and Sudhanshu Singh, 2014, Peer lending risk predictor, (<a href="http://cs229.stanford.edu/proj2014/Kevin%20Tsai,Sivagami%20Ramiah,Sudhanshu%20Singh,Peer%20Lending%20Risk%20Predictor.pdf">http://cs229.stanford.edu/proj2014/Kevin%20Tsai,Sivagami%20Ramiah,Sudhanshu%20Singh,Peer%20Lending%20Risk%20Predictor.pdf</a>).</p>
</div>
<div id="ref-mdlWu1">
<p>Wu, James, 2015, Changes in lending club underwriting,. Blog <em>MonJa</em> (<a href="https://www.monjaco.com/blog/changes-in-lending-club-underwriting-looking-beneath-the-headlines//" class="uri">https://www.monjaco.com/blog/changes-in-lending-club-underwriting-looking-beneath-the-headlines//</a>).</p>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
